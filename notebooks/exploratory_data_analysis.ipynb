{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Process:\n",
    "1. [Import dependencies and configure settings](#Step-1:-Import-dependencies-and-configure-settings)\n",
    "2. [Load the data into python](#Step-2:-Load-the-data-into-python)\n",
    "3. [Look at the raw data](#Step-3:-Look-at-the-raw-data)\n",
    "4. [Ask questions and make plots to dive deeper](#Step-4:-Ask-questions-and-make-plots-to-dive-deeper)\n",
    "5. [Outline a plan of action for arriving at the final product](#Step-5:-Outline-a-plan-of-action-for-arriving-at-the-final-product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import dependencies and configure settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages, modules, objects, or functions into the current notebook\n",
    "TIP: Avoid 'from PACKAGE import *' syntax as it clutters your namespace with objects you may not be aware of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PRACTICE: import built-in packages first\n",
    "# from PACKAGE import OBJECT lets us bring only what we need into our namespace\n",
    "from warnings import filterwarnings\n",
    "# import PACKAGE, brings the all modules into one object named PACKAGE\n",
    "import re\n",
    "\n",
    "# BEST PRACTICE: import third-party packages second\n",
    "# from PACKAGE import OBJECT as ALIAS, renames the object in our namespace\n",
    "from geopy.geocoders import Nominatim as geopy_Nominatim\n",
    "from nltk.tokenize import word_tokenize as nltk_word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer as nltk_WordNetLemmatizer\n",
    "from nltk.tag import pos_tag as nltk_pos_tag\n",
    "from pandas_profiling import ProfileReport\n",
    "from wordcloud import (\n",
    "    # use parenthesis to import multiple objects and even add aliases\n",
    "    STOPWORDS as wordcloud_STOPWORDS,\n",
    "    WordCloud\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "# aliases can be added when fully importing packages\n",
    "import pandas as pd\n",
    "# aliases can also be used when importing an individual module from the package\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "\n",
    "# BEST PRACTICE: import custom packages last\n",
    "# for example: import custom_module as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure settings by using functions or methods (always check the documentation)\n",
    "Third-party packages used in this notebook:\n",
    "- [GeoPy](https://geopy.readthedocs.io/en/stable/)\n",
    "- [Matplotlib](https://matplotlib.org/)\n",
    "- [Natural Language Toolkit](https://www.nltk.org/)\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/version/1.2.4/user_guide/index.html)\n",
    "- [Pandas Profiling](https://pandas-profiling.ydata.ai/docs/master/index.html)\n",
    "- [Plotly](https://plotly.com/python/)\n",
    "- [Seaborn](https://seaborn.pydata.org/)\n",
    "- [WordCloud](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set python shell filter out warnings and avoid cluttering outputs\n",
    "filterwarnings(\n",
    "    'ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set custom pandas package options by iterating through key-value pairs in a dictionary\n",
    "for option, value in { # dictionaries are denoted by curly brackets {} or the dict() function\n",
    "    'display.max_columns': 50,\n",
    "    'display.max_colwidth': None,\n",
    "    'display.max_info_columns': 50,\n",
    "    'display.max_rows': 20,\n",
    "    'display.precision': 4\n",
    "}.items(): # the .items() function of a dictionary lets us iterate through key, value pairs\n",
    "    # we can call a function on the variable we set for the objects we're iterating over\n",
    "    # in this case those variables are 'option', and 'value' and they represent\n",
    "    # the key, value pairs from the dictionary above\n",
    "    pd.set_option(\n",
    "        option, # this will be 'display.max_columns' etc..\n",
    "        value   # this will be 50, None, etc..\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Exploratory-Data-Analysis-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the data into python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tools that 'open' files and load their contents into objects\n",
    "NOTE: It is possible to build custom functions to read files, but not advised as there are many python libraries that can do so while providing enhanced performance and additional functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each dataset into a pandas DataFrame object\n",
    "cocoon_pharmacy_df = pd.read_csv(\n",
    "    # the read_csv function from the pandas package will read the file at this location\n",
    "    '../data/cocoon_center_pharmacy.csv'\n",
    ")\n",
    "data_literacy_df = pd.read_csv(\n",
    "    # the '..' notation indicates that parsing should begin at the parent folder \n",
    "    # of this notebook, so if this notebook is in the 'notebooks/' folder then\n",
    "    # '..' would map to the root folder of this repository (the parent of 'notebooks/') \n",
    "    '../data/data_literacy_questionnaire.csv'\n",
    ")\n",
    "data_journey_df = pd.read_csv(\n",
    "    '../data/data_journey_questionnaire.csv'\n",
    ")\n",
    "meeting_cadence_df = pd.read_csv(\n",
    "    '../data/meeting_cadence_survey.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Exploratory-Data-Analysis-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Look at the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Define a custom function to determine what is shown on-screen\n",
    "This is only advised when looking for something specific in the raw dataset like missing data or if the third-party tools are insufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example function with one mandatory and one optional parameter\n",
    "def display_with_info(\n",
    "    dataframe: pd.DataFrame,\n",
    "    name: str = None\n",
    ") -> None:\n",
    "    # print dataframe name if passed\n",
    "    if name:\n",
    "        print(\n",
    "            '=' * len(name),\n",
    "            end = '\\n'\n",
    "        )\n",
    "        print(\n",
    "            name,\n",
    "            end = '\\n' + (\n",
    "                '=' * len(name)\n",
    "            ) + '\\n\\n'\n",
    "        )\n",
    "    # display dataframe information\n",
    "    display(\n",
    "        dataframe.info()\n",
    "    )\n",
    "    # label the next output\n",
    "    print(\n",
    "        # create the illusion of a border by using hyphens\n",
    "        '-' * 14\n",
    "    )\n",
    "    print(\n",
    "        'First Ten Rows',\n",
    "        # create the illusion of a border by using hyphens\n",
    "        end = '\\n' + (\n",
    "            '-' * 14\n",
    "        )\n",
    "    )\n",
    "    # display the first ten rows of the dataframe\n",
    "    display(\n",
    "        dataframe.head(\n",
    "            5\n",
    "        )\n",
    "    )\n",
    "    # label the next output\n",
    "    print(\n",
    "        # create the illusion of a border by using hyphens\n",
    "        '\\n',\n",
    "        end = (\n",
    "            '-' * 13\n",
    "        ) + '\\n'\n",
    "    )\n",
    "    print(\n",
    "        'Last Ten Rows',\n",
    "        # create the illusion of a border by using hyphens\n",
    "        end = '\\n' + (\n",
    "            '-' * 13\n",
    "        )\n",
    "    )\n",
    "    # display the last ten rows of the dataframe\n",
    "    display(\n",
    "        dataframe.tail(\n",
    "            5\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: Placing the names of objects with data into a 'reference' object can assist when operating on mutiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary object referencing dataframes and their names\n",
    "sample_datasets = {\n",
    "    'Cocoon Center Pharmacy Items & Reviews': cocoon_pharmacy_df,\n",
    "    'Data Literacy Questionnaire': data_literacy_df,\n",
    "    'Data Journey Questionnaire': data_journey_df,\n",
    "    'Meeting Cadence Survey': meeting_cadence_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through (dataframe name: dataframe object) 'key: value' pairs \n",
    "for df_name, df_object in sample_datasets.items():\n",
    "    # display each dataframe's summary information and five (5) rows from both ends\n",
    "    display_with_info(\n",
    "        df_object,\n",
    "        df_name\n",
    "    )\n",
    "    # add a horizontal border to separate outputs\n",
    "    print(\n",
    "        '\\n' + (\n",
    "            '_' * 79\n",
    "        ),\n",
    "        end = '\\n\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use third-party tools for displaying data dynamically\n",
    "TIP: Check the [tool's documentation](https://pandas-profiling.ydata.ai/docs/master/index.html) for additional configurations and usage guides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through sample datasets again but display a 'Profile Report' this time\n",
    "for df_name, df_object in sample_datasets.items():\n",
    "    display(\n",
    "        # display the profile report with the configuration below\n",
    "        ProfileReport(\n",
    "            # use the dataframe object 'value' from the dictionary\n",
    "            df_object,\n",
    "            # create a title using the dataframe name 'key' from the dictionary\n",
    "            title = f'Profile of {df_name} DataFrame',\n",
    "            # set additional display option for the report\n",
    "            html = {\n",
    "                'navbar_show': True,\n",
    "                'style': {\n",
    "                    'primary_color': '#016ba9',\n",
    "                    'full_width': True\n",
    "                }\n",
    "            },\n",
    "            # don't show a progress bar while the report is being built\n",
    "            progress_bar = False\n",
    "        # convert to iframe to allow html to render in a notebook\n",
    "        ).to_notebook_iframe()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Exploratory-Data-Analysis-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Ask questions and make plots to dive deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Additional settings can be added at any point in the notebook, but if they are global it is best practice to place them at the top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default renderer in plotly to iframe and use online CDN\n",
    "pio.renderers.default = \"iframe_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib package to render plots within the notebook cells\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How many reviews belong to each brand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a figure object\n",
    "plt.figure(\n",
    "    # determine an initial size by passing length and width as parameters\n",
    "    figsize = (30,8)\n",
    ")\n",
    "# initialize a plot on the axis by calling seaborn's 'countplot' function\n",
    "ax = sns.countplot(\n",
    "    # use the cocoon pharmacy dataframe as the data for the plot\n",
    "    data = cocoon_pharmacy_df,\n",
    "    # use the 'brand' column for the x axis\n",
    "    x = 'brand',\n",
    "    color = 'blue'\n",
    ")\n",
    "# iterate through each subcomponent (patch) drawn on the axis\n",
    "for p in ax.patches:\n",
    "    # store the value of the subcomponent in a variable\n",
    "    x = p.get_x()\n",
    "    # store the height it's drawn to within the axis\n",
    "    height = p.get_height()\n",
    "    # store the width it's drawn to within the axis\n",
    "    width = p.get_width()\n",
    "    # if it is located below the displayed axis on the figure we can skip it\n",
    "    if pd.isnull(height):\n",
    "        pass\n",
    "    else:\n",
    "        # add the subcomponent's value at its location (label the bars)\n",
    "        ax.text(\n",
    "            x + width/2,\n",
    "            height,\n",
    "            str(height),\n",
    "            ha = 'center',\n",
    "            weight = 'bold',\n",
    "            fontsize = 18\n",
    "        )\n",
    "# rotate the labels on the x-axis and change their font size\n",
    "plt.xticks(\n",
    "    rotation = 45,\n",
    "    fontsize = 18\n",
    ")\n",
    "# change the font size for labels on the y-axis\n",
    "plt.yticks(\n",
    "    fontsize = 18\n",
    ")\n",
    "# add a label to the x-axis\n",
    "plt.xlabel(\n",
    "    'Brand',\n",
    "    fontsize = 25\n",
    ")\n",
    "# add a label to the y-axis\n",
    "plt.ylabel(\n",
    "    'Review Counts',\n",
    "    fontsize = 25\n",
    ")\n",
    "# add a title to the figure\n",
    "plt.title(\n",
    "    'Number of reviews per Brand - Total {}'.format(\n",
    "        # similar to using f' before a string, using .format replaces the {} in\n",
    "        # the string above with the contents of the object below\n",
    "        cocoon_pharmacy_df.shape[0]\n",
    "    ),\n",
    "    size = 30\n",
    ")\n",
    "# display the figure containing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many reviews belong to each product category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a figure object\n",
    "plt.figure(\n",
    "    # determine an initial size by passing length and width as parameters\n",
    "    figsize = (30,8)\n",
    ")\n",
    "# initialize a plot on the axis by calling seaborn's (sns) 'countplot' function\n",
    "ax = sns.countplot(\n",
    "    # use the cocoon pharmacy dataframe as the data for the plot\n",
    "    data = cocoon_pharmacy_df,\n",
    "    # use the 'product_cat' for the x-axis values\n",
    "    x = 'product_cat',\n",
    "    color = 'blue'\n",
    ")\n",
    "# iterate through each subcomponent (patch) drawn on the axis\n",
    "for p in ax.patches: \n",
    "    # store the value of the subcomponent in a variable\n",
    "    x = p.get_x()\n",
    "    # store the height it's drawn to within the axis\n",
    "    height = p.get_height()\n",
    "    # store the width it's drawn to within the axis\n",
    "    width = p.get_width()\n",
    "    # if it is located below the displayed axis on the figure we can skip it\n",
    "    if pd.isnull(height):\n",
    "        pass\n",
    "    else:   \n",
    "        # add the subcomponent's value at its location (label the bars)\n",
    "        ax.text(\n",
    "            x + width/2,\n",
    "            height,\n",
    "            str(height),\n",
    "            ha = 'center',\n",
    "            weight = 'bold',\n",
    "            fontsize = 18\n",
    "        )\n",
    "# rotate the labels on the x-axis and change their font size\n",
    "plt.xticks(\n",
    "    rotation = 45,\n",
    "    fontsize = 18\n",
    ")\n",
    "# change the font size for labels on the y-axis\n",
    "plt.yticks(\n",
    "    fontsize = 18\n",
    ")\n",
    "# add a label to the x-axis\n",
    "plt.xlabel(\n",
    "    'Product Category', \n",
    "    fontsize = 25\n",
    ")\n",
    "# add label to the y-axis\n",
    "plt.ylabel(\n",
    "    'Review Counts',\n",
    "    fontsize = 25\n",
    ")\n",
    "# add a title to the figure\n",
    "plt.title(\n",
    "    'Number of reviews per Product Category',\n",
    "    fontsize = 30\n",
    ")\n",
    "# display the figure containing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many reviews are there for each product category per brand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: Create a new object when performing transformations to maintain an 'original' with the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe object to view the forms grouped by category and brand\n",
    "grouped_count_df = cocoon_pharmacy_df.groupby(\n",
    "    # pass a list of columns we want to groupby\n",
    "    [\n",
    "        'product_cat',\n",
    "        'brand'\n",
    "    ]\n",
    "# slice for the 'form' column and get a count\n",
    ")['form'].count().reset_index().rename(\n",
    "    # rename the column to an appropriate label after transforming\n",
    "    {\n",
    "        'form':'Review Count'\n",
    "    },\n",
    "    # axis = 0 means apply to rows, axis = 1 means apply to columns\n",
    "    axis = 1\n",
    "# replace the sliced index (which may be out of order) with a new sequential one\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize a figure object using plotly express' bar plot function\n",
    "fig = px.bar(\n",
    "    # use the dataframe we just created above as the data source for the plot\n",
    "    grouped_count_df,\n",
    "    # use product category for the x-axis\n",
    "    x = 'product_cat',\n",
    "    # use the new review count column for the y-axis\n",
    "    y = 'Review Count', \n",
    "    # color the subplots differently based on the 'brand' column value\n",
    "    color = 'brand',\n",
    "    # use a discrete color sequence made for qualitative values \n",
    "    color_discrete_sequence = px.colors.qualitative.Dark24\n",
    ")\n",
    "# add a label to the x-axis and rotate the ticks\n",
    "fig.update_xaxes(\n",
    "    tickangle = 325,\n",
    "    title = 'Brand'\n",
    ")\n",
    "# add a label to the y-axis\n",
    "fig.update_yaxes(\n",
    "    title = 'Review Counts'\n",
    ")\n",
    "# add a title to the figure\n",
    "fig.update_layout(\n",
    "    # 'title' takes a dictionary argument so multiple properties can be passed\n",
    "    title = dict(\n",
    "        # create a text - title, key - value pair to define that property\n",
    "        text = '<b>Number of reviews per Product Category and Brand</b>',\n",
    "        # font also takes a dictionary to discern different properties\n",
    "        font = dict(\n",
    "            # in this case we only define a size property\n",
    "            size = 24\n",
    "        )\n",
    "    ),\n",
    "    # dictionaries can be created using the dict() function or {k:v} syntax\n",
    "    xaxis = {\n",
    "        # this property tells the plot to sort the xaxis in descending order\n",
    "        'categoryorder': 'total descending'\n",
    "    }\n",
    ")\n",
    "# display the figure containing the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What is the price variation between brands for each product category?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: Grouping object assignments and function calls into cells should be done with readability and debugging in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe object to view the unique values in a group of columns\n",
    "brand_product_prices = cocoon_pharmacy_df[\n",
    "    # slice the dataframe for the following list of columns\n",
    "    [\n",
    "        'brand',\n",
    "        'product_cat',\n",
    "        'form',\n",
    "        'volume',\n",
    "        'price'\n",
    "    ]\n",
    "# drop rows with duplicated entries, keeping the last by default \n",
    "].drop_duplicates() # closed parenthesis () tells functions to use default args\n",
    "# overwrite the contents of the volume column by 'assigning' a modified version\n",
    "brand_product_prices['volume'] = brand_product_prices['volume'].replace(\n",
    "    # replace values that say 'not available' with a 'null' object\n",
    "    {\n",
    "        'not available': None\n",
    "    }\n",
    ")\n",
    "# create a new column called price per ml by using existing columns\n",
    "brand_product_prices['price_per_ml'] = brand_product_prices['price'].div(\n",
    "    # use a vectorized operation to divide the prices by the volumes\n",
    "    brand_product_prices['volume'].astype(\n",
    "        # astype function converts the text values in this column to decimals\n",
    "        'float'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a figure object using plotly express' box plot function\n",
    "fig = px.box(\n",
    "    # use the dataframe we just created above as the data source\n",
    "    brand_product_prices,\n",
    "    # use brand for the x-axis\n",
    "    x = 'brand',\n",
    "    # use the new price per milliliter column for the y-axis\n",
    "    y = 'price_per_ml',\n",
    "    # color the subplots based on the 'product category' column value\n",
    "    color = 'product_cat',\n",
    "    # show all points instead of hiding outliers past a certain threshold\n",
    "    points = 'all'\n",
    ")\n",
    "# add a label to the x-axis and rotate the ticks\n",
    "fig.update_xaxes(\n",
    "    tickangle = 325,\n",
    "    title = 'Brand'\n",
    ")\n",
    "# add a label to the y-axis\n",
    "fig.update_yaxes(\n",
    "    title = 'Price per ml'\n",
    ")\n",
    "# add a title to the figure\n",
    "fig.update_layout(\n",
    "    title = 'PRICE VARIATION PER BRAND AND PRODUCT CATEGORY'\n",
    ")\n",
    "# add text to the title in the figure\n",
    "fig.update_layout(\n",
    "    # 'title' parameter takes a dictionary so multiple arguments can be passed\n",
    "    title = dict(\n",
    "        # add subtitle\n",
    "        text = '<b>Price variation per product and brand</b>',\n",
    "        # 'font' also takes a dictionary but we are just changing one property\n",
    "        font = dict(\n",
    "            size = 24\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# display the figure containing the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are customers located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up text in the customer locations column by 'redefining' the object\n",
    "cocoon_pharmacy_df['customer_loc'] = cocoon_pharmacy_df['customer_loc']\\\n",
    ".str.strip(  # remove whitespace from the ends of each string (text object)\n",
    ").str.lower( # turn all characters into lowercase\n",
    ").replace(\n",
    "    # replace vague and unused labels\n",
    "    {\n",
    "        'the kingdom':'saudi arabia',\n",
    "        'good':None,\n",
    "        'not available':None,\n",
    "        'ksa':'saudi arabia',\n",
    "        'the kingdom':'saudi arabia',\n",
    "        'london o':'london, ontario',\n",
    "        'cheshire':'cheshire, uk',\n",
    "        '-':None,\n",
    "        'sale, cheshire':'sale, cheshire, uk',\n",
    "        'tilbury, essex':'tilbury, essex, uk',\n",
    "        'middle east':'ksa middle east',\n",
    "        'kingston':'kingston, uk',\n",
    "        'awali':'awali, bahrain',\n",
    "        'riyadh, s. a.':'riyadh, saudi arabia',\n",
    "        'north':None,\n",
    "        'south east':None,\n",
    "        'memo':None\n",
    "    }\n",
    ")\n",
    "# initialize geolocator object to get coordinates from location names\n",
    "geolocator = geopy_Nominatim(\n",
    "    user_agent = \"s\"\n",
    ")\n",
    "# display example output of coordinates based on a given location name\n",
    "geolocator.geocode(\"Abu Dhabi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: The two (2) cells below do not need to be run if 'cocoon_pharmacy_location_added.csv' is present in the '/data/' folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns with coordinates for the customer locations\n",
    "new_cocoon_df = cocoon_pharmacy_df.assign(\n",
    "    Customer_Location_Latitude_Longitude = cocoon_pharmacy_df['customer_loc'].apply(\n",
    "        # lamdda denotes an anonymous function where the 'caller' is the 'input'\n",
    "        # in this case if the value is not null we want to get the 'geocode'\n",
    "        lambda x: geolocator.geocode(x) if pd.notnull(\n",
    "            geolocator.geocode(x)\n",
    "        ) else None # otherwise we leave null values alone\n",
    "    ),\n",
    "    # use the column created in the lines above to create the two columns below\n",
    "    Latitude = cocoon_pharmacy_df['Customer_Location_Latitude_Longitude'].apply(\n",
    "        lambda x: x.latitude if pd.notnull(x) else None\n",
    "    ),\n",
    "    Longitude = cocoon_pharmacy_df['Customer_Location_Latitude_Longitude'].apply(\n",
    "        lambda x: x.longitude if pd.notnull(x) else None\n",
    "    )\n",
    ")\n",
    "# get set of columns in the dataframe excluding the ones pertaining to coordinates\n",
    "# a set is a container object which only keeps unique values but does not \n",
    "# maintain its contents in the order they were placed \n",
    "columns_without_customer_location = set(\n",
    "    new_cocoon_df.columns\n",
    ").difference(\n",
    "    # we can use the difference function of sets to exclude a subset of values\n",
    "    [\n",
    "        'Customer_Location_Latitude_Longitude',\n",
    "        'Latitude',\n",
    "        'Longitude'\n",
    "    ]\n",
    ")\n",
    "# use slicing to remove  duplicates in the non-coordinate columns\n",
    "new_cocoon_df = new_cocoon_df.loc[\n",
    "    # slice the dataframe for columns without location and then drop the duplicates\n",
    "    new_cocoon_df[columns_without_customer_location].drop_duplicates(\n",
    "        # keep the first instance of a duplicate\n",
    "        keep  = 'first'\n",
    "    # use the index of this 'de-duped' dataframe to slice for the 'first' instance of each value\n",
    "    ).index,\n",
    "    # ':' by itself slices for all columns\n",
    "    :\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: Save python objects back into files to avoid recomputing time-consuming transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the modified dataframe as a .csv file in the 'data' folder\n",
    "new_cocoon_df.to_csv(\n",
    "    '../data/cocoon_pharmacy_location_added.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: The cell below can be run without the preceeding two (2) if 'cocoon_pharmacy_location_added.csv' is present in the '/data/' folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the 'cocoon_pharmacy_df' object with a dataframe of our 'new' file\n",
    "cocoon_pharmacy_df = pd.read_csv(\n",
    "    '../data/cocoon_pharmacy_location_added.csv',\n",
    "    # this dataframe was stored as .csv, with the index in the first position\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize a figure by using plotly express' geographic scatter plot function\n",
    "fig = px.scatter_geo(\n",
    "    # use the 'new' dataframe which has customer location coordinates\n",
    "    cocoon_pharmacy_df,\n",
    "    # use the column named 'Latitude' for latitude coordinates\n",
    "    lat = 'Latitude',\n",
    "    # use the column named 'Longitude' for Longitude coordinates\n",
    "    lon = 'Longitude',\n",
    "    # use customer location text to label the box that appears when hovering\n",
    "    hover_name = 'customer_loc'\n",
    ")\n",
    "# set properties for the map the plot will be drawn on\n",
    "fig.update_geos(\n",
    "    # set a resolution for the rendered image\n",
    "    resolution = 50,\n",
    "    # show land on the map\n",
    "    showland = True,\n",
    "    # make the land light green\n",
    "    landcolor = \"LightGreen\",\n",
    "    # show oseans on the map\n",
    "    showocean = True, \n",
    "    # make the oceans light blue\n",
    "    oceancolor = \"LightBlue\"\n",
    ")\n",
    "# set the margins of the dynamic figure and add a gray background\n",
    "fig.update_layout(\n",
    "    margin = dict(\n",
    "        l = 5,\n",
    "        r = 5,\n",
    "        t = 25,\n",
    "        b = 5\n",
    "    ),\n",
    "    paper_bgcolor = \"lightGrey\"\n",
    ")\n",
    "# set new subplot properties\n",
    "fig.update_traces(\n",
    "    marker = dict(\n",
    "        size = 9,\n",
    "        color = 'yellow',\n",
    "        opacity = 0.6\n",
    "    )\n",
    ")\n",
    "# add a title to the figure\n",
    "fig.update_layout(\n",
    "    title = 'Customer Locations, Cocoon Pharmacy'\n",
    ")\n",
    "# display the figure containing the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common terms used in reviews of each product category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through list of components to download from the NLTK library\n",
    "for component in [\n",
    "    'punkt',\n",
    "    'stopwords',\n",
    "    'averaged_perceptron_tagger',\n",
    "    'wordnet',\n",
    "    'omw-1.4'\n",
    "]:\n",
    "    # use nltk's download function to pull the components\n",
    "    nltk.download(\n",
    "        component\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: '\\\\' is an escape character, when used at the end of a line it extends it to include the contents of the one below (escapes newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the column containing the text body of the customer reviews\n",
    "cocoon_pharmacy_df['body_review'] = cocoon_pharmacy_df['body_review']\\\n",
    ".str.lower( # convert all the text in the reviews to lowercase \n",
    ").apply( \n",
    "    # apply an anonymous function to remove all special characters\n",
    "    lambda x:  re.sub(\n",
    "        r'[^\\w\\s]',\n",
    "        '',\n",
    "        x\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to generate a wordcloud for a given dataframe\n",
    "def make_wordcloud(\n",
    "    # takes one positional argument of Pandas DataFrame type named 'df'\n",
    "    df: pd.DataFrame\n",
    "# returns a wordcloud object\n",
    ") -> WordCloud:\n",
    "    # create a string object of all the words in all the reviews in the frame\n",
    "    all_words = ' '.join( # each review body will be separated by empty space\n",
    "        # assumes each dataframe has a column named 'body_review'\n",
    "        df['body_review']\n",
    "    )\n",
    "    # create an object of the tokens in all reviews in a dataframe, where a \n",
    "    # token is a grouping of alphanumeric characters such as a word or punctuation\n",
    "    tokenized_words = nltk_word_tokenize(\n",
    "        # this separates a continuous string of text into a list of tokens\n",
    "        all_words\n",
    "    )\n",
    "    # create set of unique english 'stopwords' (i.e. 'and', 'to', 'of')\n",
    "    # the set() function and curly brackets without 'key:value' pairs both create a set  \n",
    "    english_stops = {\n",
    "        # the asterisk '*' at the beginning unpacks the list's contents so all\n",
    "        # the items within are passed instead of just the container object\n",
    "        *nltk_stopwords.words(\n",
    "            # passing a language generates the list of all stopwords nltk has for it \n",
    "            'english'\n",
    "        )\n",
    "    }\n",
    "    # create string of all tokenized words that fit some conditions\n",
    "    all_tokens = ' '.join( # each token will be separated by empty space\n",
    "        # by using an expression we can 'generate' the contents of a list\n",
    "        [\n",
    "            # iterate through the tokens from the tokenized_words object above\n",
    "            # by placing them in the temporary variable 'token' if they satisfy conditions\n",
    "            token for token in tokenized_words if ( \n",
    "                # first condition checks tokens are longer than 2 characters\n",
    "                len(token) > 2 \n",
    "            ) and (  # and statement means both conditions must be true\n",
    "                # second condition checks tokens are not a stopword\n",
    "                token not in english_stops # not found in stopword list\n",
    "            ) and (  # chaining and statements allows us to enforce multiple\n",
    "                # third condition checks the token is not an empty string\n",
    "                pd.notna(token)\n",
    "            ) and (  # placing conditions in parenthesis is not necessary but improves readability\n",
    "                # fourth condition checks the token consists only of letters\n",
    "                token.isalpha()\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    # initialize the WordNetLemmatizer from the 'Natural Language Tool Kit'\n",
    "    lemmatizer = nltk_WordNetLemmatizer() # closed parenthesis since it's a call not an object\n",
    "    # use the 'Part of Speech' tagger to add what each word token is\n",
    "    parts_of_speech = nltk_pos_tag( # adds label to tag as 'noun', 'verb', etc.\n",
    "        # create new list of tokens from those that passed the conditions above\n",
    "        # i.e. [('word', 'wordtype'), (.., ..)] with a pair of values for each word\n",
    "        nltk_word_tokenize(\n",
    "            all_tokens\n",
    "        )\n",
    "    )\n",
    "    # initialize the final object that will contain the wordcloud text  \n",
    "    wordcloud_tokens = ''\n",
    "    # iterate through a sequence of integers of size equal to the number of tokens\n",
    "    for i in range( # generates a sequence from 0 to the number passed by default\n",
    "        # len function can return the number of objects in a container object\n",
    "        len(\n",
    "            # in this case parts_of_speech is a list of labeled tokens\n",
    "            parts_of_speech\n",
    "        )\n",
    "    ):  \n",
    "        ''' NOTE: triple quotes can be used to create a multi-line comment, like\n",
    "        - Objects defined in an indented block are temporary by default since\n",
    "        they are only present during its execution\n",
    "\n",
    "        - [] notation at the end of an object lets us access its contents if its \n",
    "        a container, although the syntax varies by type\n",
    "        ''' \n",
    "        if \"V\" in parts_of_speech[i][1][0]:\n",
    "            ''' NOTE: The line above is best read in reverse order, it states that if\n",
    "            the list in position 0 (first) of the object at position 1 (second) \n",
    "            of the object at position i (which is the value we're iterating) \n",
    "            in the parts_of_speech list contains the string 'v', then proceed to \n",
    "            the indented block below where we create a temporary object with a \n",
    "            text string ('v'). We slice the parts_of_speech list this way because \n",
    "            it is in the format [('word', 'wordtype'), (.., ..)]\n",
    "            '''\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            # otherwise we create the same object but with a value of 'n'\n",
    "            pos = 'n'\n",
    "        # use the lemmatizer object to group words with inflections together\n",
    "        lem = lemmatizer.lemmatize(\n",
    "            # takes the word itself, which is the first object in each pair in the list\n",
    "            parts_of_speech[i][0],\n",
    "            # use the 'pos' object from if/else block above to tell the lemmatizer \n",
    "            # what it needs to group each word with (i.e. nouns or verbs), converting \n",
    "            # them when possible if they're an adjective, conjuction, etc. \n",
    "            pos\n",
    "        )\n",
    "        # add the lemmatized token to the output string with a space to separate it\n",
    "        wordcloud_tokens += (lem) + ' ' # += tells python to add to the object and not overwrite it\n",
    "    # generate wordcloud and store it in an object while passing some arguments\n",
    "    wordcloud_from_reviews = WordCloud(\n",
    "        # a list of stopwords to remove from the output\n",
    "        stopwords = wordcloud_STOPWORDS,\n",
    "        # what percentage of the words should be displayed horizontally\n",
    "        prefer_horizontal = 0.9,\n",
    "        # the maximum number of words to display\n",
    "        max_words = 100\n",
    "    ).generate( # call the generate function from the object being initialized\n",
    "        # pass the tokens that have been cleaned, vetted, and lemmatized\n",
    "        wordcloud_tokens\n",
    "    )\n",
    "    # return the wordcloud object when the function is called\n",
    "    return wordcloud_from_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a figure with one row and four columns to fit multiple subplots\n",
    "fig,ax = plt.subplots(\n",
    "    nrows = 1,\n",
    "    ncols = 4,\n",
    "    figsize = (25,10)\n",
    ")\n",
    "# initialize an object to track the columns in the figure housing the wordclouds \n",
    "column_num = 0\n",
    "# iterate through a list of the top five product categories in descending order\n",
    "# and define a temporary object named 'category' to reference its contents during iteration\n",
    "for category in cocoon_pharmacy_df['product_cat'].value_counts( # counts the occurrence of each value in 'product_cat'\n",
    ").sort_values(\n",
    "    # sorts the values by descending order of their frequency\n",
    "    ascending = False\n",
    "# a colon inside brackets [:VALUE] lets us slice for all values up to (or from) \n",
    "# a position, .index grabs the labels for the rows in the dataframe (in this \n",
    "# case the categories themselves), so the line below gets the first five labels\n",
    "# from the sorted list of category frequencies we created in the lines above\n",
    ")[:4].index:\n",
    "    # display the value of the 'category' object we're iterating over\n",
    "    print(\n",
    "        category\n",
    "    )\n",
    "    # add a wordcloud to the figure at the 'column_num' column in the axis (i.e. 0 = first)  \n",
    "    ax[column_num].imshow(\n",
    "        # call the function we defined earlier to create wordclouds from a dataframe\n",
    "        make_wordcloud(\n",
    "            # pass a slice of the dataframe with all rows with the category we're iterating through\n",
    "            cocoon_pharmacy_df[cocoon_pharmacy_df['product_cat'] == category]\n",
    "        )\n",
    "    )\n",
    "    # turn off the ticks to display only the images drawn on the figure\n",
    "    ax[column_num].axis(\"off\")\n",
    "    # add a title to each image on the figure\n",
    "    ax[column_num].set_title(\n",
    "        # f' syntax lets us replace the contents of a {} with the value of an object\n",
    "        f'Wordcloud - {category}', # e.g 'Wordcloud - Body Dry Oils' if category = 'Body Dry Oils' \n",
    "        fontsize = 18\n",
    "    )\n",
    "    # increment the object so the following iteration adds a figure to the next column over\n",
    "    column_num += 1\n",
    "# remove whitespace around the edges of the image in the figure\n",
    "fig.tight_layout()\n",
    "# display the figure containing the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Exploratory-Data-Analysis-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Outline a plan of action for arriving at the final product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- Keep the [Value-Information-Analytics](https://catalent.sharepoint.com/sites/EnterpriseDataScience/SitePages/A-Methodical-Approach-to-a-Data-Centric-Problem.aspx) model in mind when determining the viability of an approach\n",
    "- Not all questions have to be answered at this point, but an understanding of the data and how it can resolve the business problem should be clear before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('dbr-10_4ml-base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "065e060c6089993c74c9bf4b911d37e59dadeed66bb03a666c87bc53eaee6720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
