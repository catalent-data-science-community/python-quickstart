{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254b391a",
   "metadata": {},
   "source": [
    "# Machine Learning with Python\n",
    "---\n",
    "---\n",
    "The Process:\n",
    "1. [Import dependencies and configure settings](#Step-1:-Import-dependencies-and-configure-settings)\n",
    "2. [Load pre-processed data](#Step-2:-Load-pre-processed-data)\n",
    "3. [Create instance of LDA model](#Step-3:-Create-instance-of-LDA-model)\n",
    "4. [Visualize model results](#Step-4:-Visualize-model-results)\n",
    "\n",
    "![topic-modeling](../images/topic_model_intro.png \"Topic Modeling Intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82102f8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b9526",
   "metadata": {},
   "source": [
    "## Step 1: Import dependencies and configure settings\n",
    "### Import packages, modules, objects, or functions into the current notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fbb91",
   "metadata": {},
   "source": [
    "***TIP: Avoid 'from PACKAGE import \\*' syntax as it clutters your namespace with objects you may not be aware of***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8862f-a34b-4504-8131-55e5e57aff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PRACTICE: import built-in packages first\n",
    "# from PACKAGE import OBJECT lets us bring only what we need into our namespace\n",
    "from pprint import pprint\n",
    "from warnings import filterwarnings\n",
    "\n",
    "# BEST PRACTICE: import third-party packages second\n",
    "# from PACKAGE import OBJECT as ALIAS, renames the object in our namespace\n",
    "from gensim.models.ldamodel import LdaModel as gensim_LdaModel\n",
    "from joblib import load as joblib_load\n",
    "from pyLDAvis.gensim_models import prepare as pyLDAvis_prepare_gensim\n",
    "from seaborn import lineplot as seaborn_lineplot\n",
    "# aliases can also be used when importing an individual module from the package\n",
    "import matplotlib.pyplot as plt\n",
    "# aliases can be added when fully importing packages\n",
    "import pandas as pd\n",
    "\n",
    "# BEST PRACTICE: import custom packages last\n",
    "# for example: import custom_module as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eb149",
   "metadata": {},
   "source": [
    "### Configure settings by using functions or methods (always check the documentation)\n",
    "Third-party packages used in this notebook:\n",
    "- [Gensim](https://radimrehurek.com/gensim/)\n",
    "- [Matplotlib](https://matplotlib.org/)\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/version/1.2.4/user_guide/index.html)\n",
    "- [pyLDAvis](https://github.com/bmabey/pyLDAvis)\n",
    "- [Seaborn](https://seaborn.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ec3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set python shell filter out warnings and avoid cluttering outputs\n",
    "filterwarnings(\n",
    "    'ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad79088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set custom pandas package options by iterating through key-value pairs in a dictionary\n",
    "for option, value in { # dictionaries are denoted by curly brackets {} or the dict() function\n",
    "    'display.max_columns': 50,\n",
    "    'display.max_colwidth': None,\n",
    "    'display.max_info_columns': 50,\n",
    "    'display.max_rows': 20,\n",
    "    'display.precision': 4\n",
    "}.items(): # the .items() function of a dictionary lets us iterate through key, value pairs\n",
    "    # we can call a function on the variable we set for the objects we're iterating over\n",
    "    # in this case those variables are 'option', and 'value' and they represent\n",
    "    # the key, value pairs from the dictionary above\n",
    "    pd.set_option(\n",
    "        option, # this will be 'display.max_columns' etc..\n",
    "        value   # this will be 50, None, etc..\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pyLDAvis to render plots within IPython notebook cells\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d3e58",
   "metadata": {},
   "source": [
    "[Top](#Machine-Learning-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76145768",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8eeb6",
   "metadata": {},
   "source": [
    "## Step 2: Load pre-processed data\n",
    "### Use [pickling](https://realpython.com/python-pickle-module/) to store python objects and reuse them across notebooks \n",
    "NOTE: Pickled files can only be 'unpickled' in an environment that matches the same dependencies as the one it was 'pickled' in, i.e. a colleague wouldn't be able to open pickled files if they don't have the same python environment on their machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0965319",
   "metadata": {},
   "source": [
    "### Topic Model Requirements\n",
    "\n",
    "**id2word**\n",
    ": A python dictionary mapping the numerical identifiers created during pre-processing to the actual words they represent\n",
    "\n",
    "**corpus**\n",
    ": A 'bag of words' representation of each review in the dataset, converting text to a python list of tuples *--immutable objects--* in the form of (Word Identifier, Word Frequency)\n",
    "\n",
    "NOTE: We use the Gensim package's functions to transform the text for us in the [Data Wrangling Notebook](./data_wrangling.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpus and id2words objects by 'unpickling' them\n",
    "corpus = joblib_load(\n",
    "    '../data/cocoon_reviews_corpus.pkl'\n",
    ")\n",
    "id2word = joblib_load(\n",
    "    '../data/cocoon_reviews_id2word.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffbaef",
   "metadata": {},
   "source": [
    "### Text Pre-processing Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fbbff",
   "metadata": {},
   "source": [
    "**1. Tokenization**: Split raw text into small, indivisible units for processing.\n",
    "These can be:\n",
    "- words\n",
    "- n-grams\n",
    "- sentences\n",
    "- paragraphs\n",
    "\n",
    ">> n-gram\n",
    ": A sequence of *n* number of words, i.e. a 2-gram, called a bigram, is a sequence of two words (“please turn”, “turn your”, or ”your homework”), and a 3-gram, called a trigram, is a three-word sequence of words (“please turn your”, or “turn your homework”)\n",
    "\n",
    "**2. Normalization**: Convert all text to lower case, expand contractions, remove numerals and accent marks\n",
    "\n",
    ">> Contraction\n",
    ": A word made by shortening and combining two words, i.e. can't (can + not), don't (do + not), and I've (I + have)\n",
    "\n",
    "**3. Stopwords Removal**: Remove words below a certain length threshold or which contribute little overall meaning\n",
    "\n",
    ">> Stopwords\n",
    ": Sets of commonly used words in given language, whose removal will increase overall meaning garnered from the text as only important or relevant words will remain\n",
    "\n",
    "**4. Stemming or Lemmatization**: Tag each word by the part of speech it represent, and then eliminate affixes from a word by removing common word endings ('-ing', '-ed'), or capturing canonical forms based on a word's lemma (chosen representative)\n",
    "\n",
    "![stemming_vs_lemmatization](../images/stemming_vs_lemmatization.png)\n",
    "\n",
    ">> Stemming\n",
    ": Cut off the end or the beginning of the word based on an algorithm, typically taking into account a list of common prefixes and suffixes that can be found in an inflected word in a given language\n",
    "\n",
    ">> Lemmatization\n",
    ": Grouping words based on a morphological analysis of it's structure and definition. To do so it is necessary to have detailed dictionaries of words and their representatives, which the algorithms can look through to link the form back to a lemma.\n",
    "\n",
    ">> Parth-of-Speech Tagging\n",
    ": A popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae8d1d",
   "metadata": {},
   "source": [
    "### Why is part of speech tagging important to do before stemming or lemmatization?\n",
    "![pos_example1](../images/pos_example1.png)\n",
    "![pos_example2](../images/pos_example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d3e58",
   "metadata": {},
   "source": [
    "[Top](#Machine-Learning-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76145768",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103ac17",
   "metadata": {},
   "source": [
    "## Step 3: Create instance of LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d4836",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation (LDA)**  \n",
    "\n",
    "A ‘generative probabilistic model’, seeking to allow sets of observations to be described by unobserved groups that explain why some parts of the data are similar. \n",
    "\n",
    "For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word’s presence is attributable to one of the document’s topics. In this case the documents are the reviews and the parts are the words and/or phrases (n-grams), with LDA serving as way of soft clustering the documents and parts. \n",
    "\n",
    "The fuzzy memberships drawn provide a more nuanced way of inferring topics, as each review can be made up of a combination of topics, versus simply attributing each review to one topic. This feature of LDA is key in this instance, as we are seeking to draw out terms indicative of constructive comments, with the assumption that customers might write about more than one topic in their reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a8f13",
   "metadata": {},
   "source": [
    "### Optional\n",
    "Tune the [hyperparameters](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac), which are paramaters whode values control the learning process and determine the values of model parameters that a learning algorithm ends up learning  \n",
    "\n",
    "For this Latent Dirichlet Allocation Model those would be:\n",
    "- num_topics\n",
    "- random_state\n",
    "- update_every\n",
    "- chunksize\n",
    "- passes\n",
    "- alpha\n",
    "- per_word_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac38fa3-7ce1-4eaf-8438-ade06b2a996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = gensim_LdaModel(\n",
    "    corpus = corpus,\n",
    "    id2word = id2word,\n",
    "    num_topics = 4,\n",
    "    random_state = 100,\n",
    "    update_every = 1,\n",
    "    chunksize = 100,\n",
    "    passes = 10,\n",
    "    alpha = 'auto',\n",
    "    per_word_topics = True\n",
    ")\n",
    "model_topics = optimal_model.show_topics(\n",
    "    formatted = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d3e58",
   "metadata": {},
   "source": [
    "[Top](#Machine-Learning-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76145768",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a023b",
   "metadata": {},
   "source": [
    "## Step 4: Visualize the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d98286-844b-40c7-8e8f-84b869966b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(optimal_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374057d-2a86-417a-a31f-f5aab450429a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_n_words = 10\n",
    "topics = optimal_model.show_topics(\n",
    "    num_topics =4, num_words = top_n_words, formatted = False)\n",
    "\n",
    "for _, infos in topics:\n",
    "    probs = [prob for _, prob in infos]\n",
    "    seaborn_lineplot(range(top_n_words), probs, marker = '*')\n",
    "\n",
    "plt.xlabel('Word rank')\n",
    "plt.ylabel('Weights')\n",
    "plt.title('Weights of Top {} Words in each Topic'.format(top_n_words))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd79996-370b-49c3-9347-1ec302fda26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = pyLDAvis_prepare_gensim(optimal_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d3e58",
   "metadata": {},
   "source": [
    "[Top](#Machine-Learning-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76145768",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dbr-10_4ml-base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "065e060c6089993c74c9bf4b911d37e59dadeed66bb03a666c87bc53eaee6720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
