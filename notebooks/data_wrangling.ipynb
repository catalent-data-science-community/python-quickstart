{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Python\n",
    "---\n",
    "---\n",
    "The Process:\n",
    "1. [Import dependencies and configure settings](#Step-1:-Import-dependencies-and-configure-settings)\n",
    "2. [Load the data into python](#Step-2:-Load-the-data-into-python)\n",
    "3. [Create data views or new datasets by making more python objects](#Step-3:-Create-data-views-or-new-datasets-by-making-more-python-objects)\n",
    "4. [Lock down the process and store the final result in a reliable location](#Step-4:-Lock-down-the-process-and-store-the-final-result-in-a-reliable-location)\n",
    "\n",
    "![Lego-Data](../images/data_wrangling.jpg \"Lego Data Wrangling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import dependencies and configure settings\n",
    "### Import packages, modules, objects, or functions into the current notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TIP: Avoid 'from PACKAGE import \\*' syntax as it clutters your namespace with objects you may not be aware of***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PRACTICE: import built-in packages first\n",
    "# from PACKAGE import OBJECT lets us bring only what we need into our namespace\n",
    "from string import punctuation\n",
    "from warnings import filterwarnings\n",
    "# import PACKAGE, brings the all modules into one object named PACKAGE\n",
    "import re\n",
    "\n",
    "# BEST PRACTICE: import third-party packages second\n",
    "# import PACKAGE as ALIAS, places all the modules into one object named ALIAS\n",
    "from contractions import fix as contractions_fix\n",
    "from gensim.corpora import Dictionary as gensim_Dictionary\n",
    "from gensim.models import Phrases as gensim_Phrases\n",
    "from gensim.models.phrases import Phraser as gensim_Phraser\n",
    "from geopy.geocoders import Nominatim as geopy_Nominatim\n",
    "from joblib import dump as joblib_dump\n",
    "from nltk import pos_tag as nltk_pos_tag\n",
    "from nltk.tokenize import word_tokenize as nltk_word_tokenize\n",
    "from nltk.corpus import (\n",
    "    # use parenthesis to import multiple objects and even add aliases\n",
    "    stopwords as nltk_stopwords,\n",
    "    wordnet as nltk_wordnet\n",
    ")\n",
    "from nltk.stem import (\n",
    "    SnowballStemmer as nltk_SnowballStemmer,\n",
    "    WordNetLemmatizer as nltk_WordNetLemmatizer\n",
    ")\n",
    "from nltk.tag import pos_tag as nltk_pos_tag\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer as sklearn_CountVectorizer,\n",
    "    TfidfTransformer as sklearn_TfidfTransformer,\n",
    "    TfidfVectorizer as sklearn_TfidfVectorizer\n",
    ")\n",
    "# aliases can be added when fully importing packages\n",
    "import bamboolib as bam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "# aliases can also be used when importing an individual module from the package\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# BEST PRACTICE: import custom packages last\n",
    "# for example: import custom_module as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure settings by using functions or methods (always check the documentation)\n",
    "Third-party packages used in this notebook:\n",
    "- [Bamboolib](https://docs.bamboolib.8080labs.com/documentation/getting-started)\n",
    "- [Contractions](https://github.com/kootenpv/contractions)\n",
    "- [Gensim](https://radimrehurek.com/gensim/)\n",
    "- [Geopy](https://github.com/geopy/geopy)\n",
    "- [Matplotlib](https://matplotlib.org/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [NumPy](https://numpy.org/doc/stable/user/index.html)\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/version/1.2.4/user_guide/index.html)\n",
    "- [pyLDAvis](https://github.com/bmabey/pyLDAvis)\n",
    "- [Scikit Learn](https://scikit-learn.org/stable/)\n",
    "- [SciPy](https://docs.scipy.org/doc/scipy/tutorial/index.html)\n",
    "- [Statsmodels](https://www.statsmodels.org/devel/user-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set python shell filter out warnings and avoid cluttering outputs\n",
    "filterwarnings(\n",
    "    'ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set custom pandas package options by iterating through key-value pairs in a dictionary\n",
    "for option, value in { # dictionaries are denoted by curly brackets {} or the dict() function\n",
    "    'display.max_columns': 50,\n",
    "    'display.max_colwidth': None,\n",
    "    'display.max_info_columns': 50,\n",
    "    'display.max_rows': 20,\n",
    "    'display.precision': 4\n",
    "}.items(): # the .items() function of a dictionary lets us iterate through key, value pairs\n",
    "    # we can call a function on the variable we set for the objects we're iterating over\n",
    "    # in this case those variables are 'option', and 'value' and they represent\n",
    "    # the key, value pairs from the dictionary above\n",
    "    pd.set_option(\n",
    "        option, # this will be 'display.max_columns' etc..\n",
    "        value   # this will be 50, None, etc..\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Data-Wrangling-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the data into python\n",
    "### Use tools that 'open' files and load their contents into objects\n",
    "NOTE: It is possible to build custom functions to read files, but not advised as there are many python libraries that can do so while providing enhanced performance and additional functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each dataset into a pandas DataFrame object\n",
    "cocoon_pharmacy_df = pd.read_csv(\n",
    "    # the read_csv function from the pandas package will read the file at this location\n",
    "    '../data/cocoon_pharmacy_location_added.csv',\n",
    "    # this .csv was saved from a dataframe with the index in the first position\n",
    "    index_col = 0\n",
    ")\n",
    "data_literacy_df = pd.read_csv(\n",
    "    # the '..' notation indicates that parsing should begin at the parent folder \n",
    "    # of this notebook, so if this notebook is in the 'notebooks/' folder then\n",
    "    # '..' would map to the root folder of this repository (the parent of 'notebooks/') \n",
    "    '../data/data_literacy_questionnaire.csv'\n",
    ")\n",
    "data_journey_df = pd.read_csv(\n",
    "    '../data/data_journey_questionnaire.csv'\n",
    ")\n",
    "meeting_cadence_df = pd.read_csv(\n",
    "    '../data/meeting_cadence_survey.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Data-Wrangling-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create data views or new datasets by making more python objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Use third-party tools for transforming data\n",
    "Bamboolib is our recommendation as it offers a great bridge into python, but any low/no-code solutions are a great place to get started with these concepts, and many such as JMP from SAS, Minitab, and Power BI also offer scripting integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Started with [Bamboolib](https://docs.bamboolib.8080labs.com/documentation/getting-started):**\n",
    "- [x] import the package *(completed at the beginning of this notebook)*\n",
    "\n",
    "- [x] create a Pandas DataFrame *(completed in the code cell above)*\n",
    "\n",
    "- [ ] display a pandas dataframe to make \"Show Bamboolib UI\" button available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TIP: DataFrames can be displayed by using the display() function or by placing the object on the last line of a cell***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Bamboolib will overwrite the contents of this cell after work is complete \n",
    "# with any code necessary to execute the requested transformations or plots\n",
    "cocoon_pharmacy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Bamboolib will overwrite the contents of this cell after work is complete \n",
    "# with any code necessary to execute the requested transformations or plots\n",
    "data_literacy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Bamboolib will overwrite the contents of this cell after work is complete \n",
    "# with any code necessary to execute the requested transformations or plots\n",
    "data_journey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Bamboolib will overwrite the contents of this cell after work is complete \n",
    "# with any code necessary to execute the requested transformations or plots\n",
    "meeting_cadence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Write code to transform the data\n",
    "Use the many packages available in Python to build flexible 'templated' solutions that address re-occurring issues or dynamic problems that require more complex integrations and/or additional capabilities *(Ex. networking, containerization/virtualization, multi-processing)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Started with writing 'Production' code:**\n",
    "- [x] Adhere to a consistent method for structuring and writing code *(see [PEP8](https://peps.python.org/pep-0008/) for more details)*\n",
    "\n",
    "- [x] Document third-party packages and the versions being used *(Ex. binder/requirements.txt file in this repository)*\n",
    "\n",
    "- [ ] Always test your code! *(Doesn't have to be automated, but 'good' code should always have error handling and logging)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TIP: Check out the [Official Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) for a Quick Syntax Reference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_moisturizers = cocoon_pharmacy_df[cocoon_pharmacy_df['product_cat'] == 'Body aMoisturisers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "  if treebank_tag.startswith('J'):\n",
    "      return nltk_wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "      return nltk_wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "      return nltk_wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "      return nltk_wordnet.ADV\n",
    "  else:\n",
    "      return nltk_wordnet.NOUN  \n",
    "    \n",
    "def first_preprocessing(pdf):\n",
    "  stopwords = nltk_stopwords.words('english')\n",
    "\n",
    "  eng = pdf.copy(deep = True)\n",
    "  \n",
    "  eng['body_review_cleaned'] = eng['body_review'].apply(lambda x: contractions_fix(x.lower().strip())) #lower case, expand contractions, and strip spaces\n",
    "  eng['body_review_cleaned'] = eng['body_review_cleaned'].apply(lambda x: ' '.join(s for s in x.split() if not any(c.isdigit() for c in s))) #remove anyword containing a digit\n",
    "  eng['body_review_cleaned'] = eng['body_review_cleaned'].apply(lambda x: \"\".join([char for char in x if char not in punctuation])) #remove punctuations\n",
    "  eng['body_review_cleaned'] = eng['body_review_cleaned'].apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords])) #remove stopwords\n",
    "  eng['body_review_cleaned'] = eng['body_review_cleaned'].apply(lambda x: re.sub(\"(\\\")?(\\')?(“)?(”)?\",'',x)) #remove “ and ”\n",
    "  eng['body_review_cleaned'] = eng['body_review_cleaned'].apply(lambda x: ' '.join([nltk_WordNetLemmatizer().lemmatize(word,get_wordnet_pos(pos_tag)) for pos in [nltk_pos_tag(x.split())]  for (word,pos_tag) in pos]))\n",
    "  \n",
    "  sentence_stream = [doc.split(\" \") for doc in eng['body_review_cleaned'].values]\n",
    "  \n",
    "  bigram = gensim_Phrases(sentence_stream, min_count=15, threshold=5) # higher threshold fewer phrases.\n",
    "  trigram = gensim_Phrases(bigram[sentence_stream], min_count=15,threshold=5)  \n",
    "\n",
    "  # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "  bigram_mod = gensim_Phraser(bigram)\n",
    "  trigram_mod = gensim_Phraser(trigram)\n",
    "  \n",
    "  return pdf, eng, sentence_stream, bigram_mod, trigram_mod\n",
    "  \n",
    "def later_preprocessing(text, bigram_mod, trigram_mod):\n",
    "    \n",
    "    text = bigram_mod[text.split()] #bigram\n",
    "    text = trigram_mod[bigram_mod[text]] #trigram\n",
    "    text = ' '.join([w.strip() for w in text if len(w.strip()) > 2 and w.strip() not in ['no','qc']]) # remove short words\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efe2d7-78ca-4295-9fcb-625c38d219b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, eng, sentence_stream, bigram_mod, trigram_mod = first_preprocessing(cocoon_pharmacy_df)\n",
    "eng['body_review_cleaned'] = eng['body_review_cleaned'].apply(lambda x: later_preprocessing(x,bigram_mod, trigram_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdfffe-f2a0-4427-a4af-601a1eab275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_streams = [doc.split(\" \") for doc in eng['body_review_cleaned'].values]\n",
    "id2word = gensim_Dictionary(sentence_streams)\n",
    "id2word.filter_extremes(no_below=7, no_above=0.95, keep_n=25000)\n",
    "corpus = [id2word.doc2bow(text) for text in sentence_streams]\n",
    "print('Total number of unique words after filtering extremes: {}'.format(len(id2word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_dump(\n",
    "    corpus,\n",
    "    '../data/cocoon_reviews_corpus.pkl'\n",
    ")\n",
    "joblib_dump(\n",
    "    id2word,\n",
    "    '../data/cocoon_reviews_id2word.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Data-Wrangling-with-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Lock down the process and store the final result in a reliable location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Always save the data to a secure location, whether that's as a table in a relational database, as a flat file in an online file-system, or as a flat file in a physical drive that is that is consistently backed up *([See Pandas In/Out Methods](https://pandas.pydata.org/docs/reference/io.html))*\n",
    "\n",
    "- Whether or not a low-code tool was used, if any scripting/programming has been built, always try to follow best practice by 'committing' it to an online [Git](https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F) Repository so that it can be securely backed up with an audit trail *([GitHub](https://github.com/) and [Azure DevOps Services](https://azure.microsoft.com/en-us/services/devops/) are both free for small teams)*\n",
    "\n",
    "- Make sure at any given time there is a good version of the code that 'always builds', meaning any future development work should occur on 'branches' (or copies) of the code before finalizing those changes on the good version *([see some methods for how to organize your development](https://medium.com/@patrickporto/4-branching-workflows-for-git-30d0aaee7bf))*\n",
    "\n",
    "- There is no such thing as documentation that is 'too descriptive'! It's highly unlikely it will ever be too much for any prospective users/contributors of bespoke code, and it diminishes the probability that the writer will tragically forget how exactly their code works *([which happens to the best of us](https://javascript.plainenglish.io/is-it-normal-to-forget-how-your-own-code-works-e5a6462f3571))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dbr-10_4ml-base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "065e060c6089993c74c9bf4b911d37e59dadeed66bb03a666c87bc53eaee6720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
